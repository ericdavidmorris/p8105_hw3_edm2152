---
title: "HW3"
author: "Eric Morris"
date: "10/5/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)

# install.packages("devtools")
devtools::install_github("p8105/p8105.datasets")
```


## Problem 1 

Loading in the BRFSS Data: 

```{r brfss_data_import}
library(p8105.datasets)

brfss_data = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  select(-class, -topic, -question, -sample_size, -(confidence_limit_low:geo_location)) %>% 
  mutate(response = forcats::fct_relevel(response, c("Excellent", "Very good", "Good", "Fair", "Poor"))) %>% 
  rename(state = locationabbr, county = locationdesc)
```

In 2002, which states were observed at 7 locations?

```{r seven_locations}
brfss_data %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  distinct(county) %>% 
  summarize(n_state = n()) %>% 
  filter(n_state == 7) %>% 
  knitr::kable()
```

Connecticut, Florida and North Carolina were observed at 7 locations.

Make a “spaghetti plot” that shows the number of observations in each state from 2002 to 2010.

```{r spaghetti}
brfss_data %>% 
  distinct(state, county, year) %>% 
  group_by(state, year) %>% 
  summarize(n_locations = n()) %>% 
  ggplot(aes(x = year, y = n_locations, color = state)) + 
  geom_line()
```

Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

```{r tables}
brfss_data %>% 
  filter(year == 2002 | year == 2006 | year == 2010, state =="NY") %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  group_by(state, year) %>% 
  summarize(mean_excellent = mean(excellent, na.rm = TRUE),
            sd_excellent = sd(excellent, na.rm = TRUE)) %>% 
  knitr::kable()
```


For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r avg_prop}
brfss_data %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  group_by(state, year) %>% 
  summarize(mean_excellent = mean(excellent, na.rm = TRUE),
            mean_verygood = mean(very_good, na.rm = TRUE),
            mean_good = mean(good, na.rm = TRUE),
            mean_fair = mean(fair, na.rm = TRUE),
            mean_poor = mean(poor, na.rm = TRUE)) %>% 
  gather(key = mean_variable, value = mean_value, mean_excellent:mean_poor) %>% 
  ggplot(aes(x = year, y = mean_value, color = state)) +
  geom_line() +
  facet_grid(~mean_variable)   
```

## Problem 2

Instacart Data Import

```{r}
instacart_data = instacart %>% 
  janitor::clean_names()
  
unique_users = instacart_data %>% 
  distinct(user_id) %>% 
  nrow()

distinct_dept = instacart_data %>% 
  distinct(department) %>% 
  nrow()

distinct_products = instacart_data %>% 
  distinct(product_id) %>% 
  nrow()
```

In the instacart dataset there are `r ncol(instacart)` variables which include identifiers for aisle, order, product and customer along with a corresponding numerical ID in some cases (aisle and product for example). The instacart dataset also includes `r nrow(instacart)` observations which is equivalent to a product from an order. There are `r unique_users` unique users in the dataset that submitted an order, `r distinct_dept` distinct departments that recieved an order, and `r distinct_products` distinct products ordered. 


```{r aisles}
distinct_aisles = 
  instacart_data %>% 
  distinct(aisle_id) %>% 
  nrow()

instacart_data %>% 
  group_by(aisle_id, aisle) %>% 
  summarize(most_aisles = n()) %>% 
  ungroup() %>% 
  top_n(5, most_aisles) %>% 
  arrange(desc(most_aisles)) %>% 
  knitr::kable()
```

There are `r distinct_aisles` distinct aisles in the dataset and the top five aisles with the most items ordered from are: 

* Fresh Vegetables (150,609 items ordered)

* Fresh Fruits (150,473 items ordered)

* Packaged Vegetables & Fruits (78,493 items ordered)

* Yogurt (55,240 items ordered)

* Packaged Cheese (41,699 items ordered)


Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

```{r items plot}
instacart_data %>% 
  group_by(aisle_id, aisle) %>% 
  summarize(number_items = n()) %>% 
  arrange(desc(number_items)) %>% 
  ggplot(aes(x = reorder(aisle, -number_items), y = number_items, fill = aisle)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 50, hjust = 1, size = 8),
        legend.position = "none")
```


The table below shoes the most popular item in the following three aisles:

* Baking Ingredients
* Dog Food Care
* Packaged Vegetables/Fruits 

```{r popular table}
instacart_data %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(number_items = n()) %>% 
  top_n(1, number_items) %>% 
  arrange(desc(number_items)) %>% 
  knitr::kable()
```


Below is a table showing the mean hour of the day (24h clock) Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week (0-6 columns):

```{r mean table}
instacart_data %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  select(product_name, order_dow, order_hour_of_day) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hour) %>% 
  knitr::kable()
```

## Problem 3


